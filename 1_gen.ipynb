{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic dataset generation using Llama 3.1 405B and RAFT\n",
    "\n",
    "This recipe will walk you through using [Meta Llama 3.1 405B](https://aka.ms/c/model/Meta-Llama-3.1-405B-Instruct) deployed on Azure AI to generate a synthetic dataset using UC Berkeley's Gorilla project RAFT method (see [blog post](https://aka.ms/raft-blog))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RAFT?\n",
    "\n",
    "RAFT stands for Retrieval Augmented Fine Tuning. The general principle is to use a big LLM such as Llama 3.1 405B to analyse a set of documents and generate a dataset of questions and answers that users might want to ask about those documents. We can then use that QA dataset to fine tune a smaller model such as Llama 3.1 8B. The fine tune model will therefore be better at answering questions about those documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogy: How to prepare a LLM for an Exam? üìù\n",
    "\n",
    "*Note: This description was copied from the [UC Berkeley RAFT blog post](https://aka.ms/raft-blog-ucb).*\n",
    "\n",
    "RAFT is a general recipe to finetune a pretrained LLM to your domain-specific RAG settings. This is a common scenario where you want your LLM to answer questions grounded on a set of documents, for e.g., private files in an enterprise. Such a setting is different from the general RAG where the LLM does not know which domain (of documents) it will be tested on. To better illustrate this setting, let's draw an analogy between deploying and using an LLM with the real-world setting of prepararing for an exam.\n",
    "\n",
    "![RAFT Open book principle](./doc/raft_openbook.png \"RAFT Open book principle\")\n",
    "\n",
    "#### Closed-Book Exam\n",
    "\n",
    "A closed book exam often refers to the scenario where the LLMs do not have access to any additional documents or references to answer the questions during the exam. For LLMs, this is equivalent to the scenario, for example, in which the LLM is used as a chatbot. In this scenario the LLM draws from the knowledge baked in during pre-training and supervised-finetuning to respond to the users' prompt.\n",
    "\n",
    "#### Open-Book Exam\n",
    "\n",
    "In contrast, we liken the open-book exam setting to the scenario in which the LLM can refer to external sources of information (e.g., a website or a book chapter). In such scenarios, typically, the LLM is paired with retriever which retrieves k documents (or specific segments of the document) which are appended to the users' prompt. It is only through these documents retrieved that the LLM gains access to new knowledge. As a result, we argue that the LLM's performance in these settings, where it is trained as a general-purpose LLM is largely dependent on the quality of the retriever and how accurately the retriever can identify the most relevant piece of information.\n",
    "\n",
    "#### RAFT\n",
    "\n",
    "RAFT focuses on a narrower but increasingly popular domain than the general open book exam, called the domain-specific open-book exam. In domain-specific open book exam, we know a priori the domain in which the LLM will be tested --- used for inference. The LLM can respond to the users' prompt using use any and all information from this specific domain, which it has been fine-tuned on. Examples of domain specific examples include enterprise documents, latest news, code repositories belonging to an organization, etc. In all these scenarios, the LLM will be used to respond to the questions, whose answers can be found within a collection of documents (a small practical domain). The retrieval technique itself has little to no-impact on the mechanism (though it may impact the accuracy). This paper mainly studies this, domain-specific open-book setting and how to adapt a pretrained LLM to this specific domain, including how to make it more robust to a varying number of retrieved documents and distractors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAFT Process: from domain documents to Q/A/CoT dataset splits\n",
    "\n",
    "The process is the following. RAFT takes as input a set of documents, split them into chunks, and for each chunk generates a list of questions, Chain Of Thought answers with a selection of relevant and irrelevant context chunks.\n",
    "\n",
    "![RAFT](./doc/raft.png \"RAFT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running time and cost\n",
    "\n",
    "The RAFT script usually takes a few minutes on the default sample document but can take days on bigger domains depending on the number and size of documents and the number of questions being generated for each chunk.\n",
    "\n",
    "The cost of running this RAFT script on the sample document should be a few dollars. But beware, running it on bigger domains can cost hundreds of dollars if not more. It is safe to run this notebook multiple times though as the costly part, running the `raft.py` script, will only be executed if the dataset doesn't exist yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "Before running this notebook, let's make sure your environment is ready\n",
    "\n",
    "### 1. Deploy Meta Llama 3.1 405B Instruct as a serverless endpoint.\n",
    "\n",
    "This model will be used to generate the synthetic dataset.\n",
    "\n",
    "You can either use [Azure ML Studio](https://aka.ms/raft-llama-31-learn-deploy-405b) or [Azure AI Studio](https://aka.ms/raft-llama-31-learn-deploy-405b-ai-studio).\n",
    "\n",
    "**Note**: an Azure ML Workspace is the same as a Azure AI Hub, you will be able to go back and forth between the two transparently.\n",
    "\n",
    "### 2. Deploy OpenAI's `text-embedding-ada-002` as a serverless endpoint.\n",
    "\n",
    "This model will be used to create the chunk embeddings.\n",
    "\n",
    "You can follow the same procedure as for the Meta Llama model deployment\n",
    "\n",
    "### 3. Setup your environment variables\n",
    "\n",
    "Copy the `.env.sample` file to `.env` and update according to your Azure AI project configuration and deployed endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the RAFT repository\n",
    " \n",
    "This script will checkout a shallow and narrow clone of the UC Berkeley Gorilla RAFT repository locally so that this notebook can invoke the RAFT script and util functions. It can safely be run multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Gorilla RAFT in /workspaces/llama-raft-recipe/.gorilla\n",
      "Your branch is up to date with 'origin/raft-distillation-recipe'.\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "! ./setup_raft.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requirements\n",
    "\n",
    "The requirements should have been automatically installed if you opened the project in Dev Container or Codespaces, but if not, uncomment the following cell to install the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check instrastructure requirements\n",
    "\n",
    "The following integration tests check that the endpoints are accessible and working before executing the RAFT program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.12.4, pytest-8.1.2, pluggy-1.5.0\n",
      "rootdir: /workspaces/llama-raft-recipe/infra/tests\n",
      "configfile: ../../pyproject.toml\n",
      "plugins: anyio-4.4.0\n",
      "collected 3 items                                                              \u001b[0m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\n",
      "\n",
      "infra/tests/test_baseline.py \u001b[32m.\u001b[0m\u001b[32m                                           [ 33%]\u001b[0m\n",
      "infra/tests/test_embeddings.py \u001b[31mF\u001b[0m\u001b[31m                                         [ 66%]\u001b[0m\n",
      "infra/tests/test_teacher.py \u001b[32m.\u001b[0m\u001b[31m                                            [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________________ test_embeddings ________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_embeddings\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfrom\u001b[39;49;00m \u001b[04m\u001b[96mopenai\u001b[39;49;00m \u001b[94mimport\u001b[39;49;00m AzureOpenAI\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Authenticate using the default Azure credential chain\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        azure_credential = DefaultAzureCredential()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = getenv(\u001b[33m\"\u001b[39;49;00m\u001b[33mEMBEDDING_AZURE_OPENAI_DEPLOYMENT\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m model \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert None is not None\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31minfra/tests/test_embeddings.py\u001b[0m:17: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m infra/tests/test_embeddings.py::\u001b[1mtest_embeddings\u001b[0m - assert None is not None\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m2 passed\u001b[0m\u001b[31m in 12.51s\u001b[0m\u001b[31m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python -m pytest --rootdir=infra/tests/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook parameters\n",
    "\n",
    "*Note: Parameters are typed as indicated for Papermill introspection*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "ds_name: str = \"surfing-1k\"\n",
    "doc_path: str = \"sample_data/surfing/\"\n",
    "format: str = \"chat\"\n",
    "finetuning_train_split : int = .8\n",
    "finetuning_valid_split : int = .1\n",
    "finetuning_threshold : int = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating state file with DATASET_NAME=surfing-1k\n",
      "Creating dataset: surfing-1k\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils import update_state\n",
    "\n",
    "ds_path = f\"dataset/{ds_name}\"\n",
    "ds_output_file = f\"{ds_path}.jsonl\"\n",
    "update_state(\"DATASET_NAME\", ds_name)\n",
    "print(\"Creating dataset: \" + ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_pdf_image\n",
    "from pathlib import Path\n",
    "\n",
    "pdf_image = None\n",
    "if Path(doc_path).exists() and Path(doc_path).is_file() and Path(doc_path).suffix == \".pdf\":\n",
    "    pdf_image = get_pdf_image(doc_path)\n",
    "pdf_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Q/A/CoT fine-tuning dataset using RAFT from the domain specific documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `--completion_model` and `--embedding_model` parameters refer to the names of the deployments of the models in Azure.\n",
    "\n",
    "**Note about `--qa-threshold`**: The Azure AI Finetuning service requires a minimum of 65 samples in the training split so in order to make this notebook run as quickly as possible with a demo dataset, we calculate the minimum number of samples we need to generate overall using the `finetuning_threshold` and the `finetuning_train_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA threshold: 1250\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "qa_threshold = ceil(finetuning_threshold / finetuning_train_split)\n",
    "print(f\"QA threshold: {qa_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "\u001b[32m2024-09-12 23:47:08\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34menv_config\u001b[0m Resolved OpenAI env vars with 'COMPLETION' prefix:\n",
      "\u001b[32m2024-09-12 23:47:08\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34menv_config\u001b[0m  - OPENAI_TYPE=\"azure\"\n",
      "\u001b[32m2024-09-12 23:47:08\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34menv_config\u001b[0m  - OPENAI_API_KEY=............................Lkxd\n",
      "\u001b[32m2024-09-12 23:47:08\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34menv_config\u001b[0m  - OPENAI_BASE_URL=https://Meta-Llama-3-1-405B-Instruct-raft.westus3.models.ai.azure.com/v1\n",
      "\u001b[32m2024-09-12 23:47:08\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34menv_config\u001b[0m  - OPENAI_DEPLOYMENT=Meta-Llama-3-1-405B-Instruct-raft\n",
      "\u001b[32m2024-09-12 23:47:08\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Using checkpoint chunks /workspaces/llama-raft-recipe/dataset/surfing-1k-checkpoints/chunks\n",
      "\u001b[32m2024-09-12 23:47:08\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Retrieving chunks from sample_data/surfing of type pdf using the text-embedding-ada-002 model.\n",
      "\u001b[32m2024-09-12 23:47:08\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34menv_config\u001b[0m Resolved OpenAI env vars with 'EMBEDDING' prefix:\n",
      "\u001b[32m2024-09-12 23:47:08\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34menv_config\u001b[0m  - OPENAI_TYPE=\"azure\"\n",
      "\u001b[32m2024-09-12 23:47:08\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34menv_config\u001b[0m  - OPENAI_API_KEY=................................................................................................................................................................f6YA\n",
      "Chunking:   0%|                                         | 0/9 [00:00<?, ?file/s]\u001b[32m2024-09-12 23:47:09\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /embeddings in 0.830368 seconds\n",
      "Chunking: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:08<00:00,  1.05file/s, chunks=544]\n",
      "\u001b[32m2024-09-12 23:47:17\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Using system prompt key llama\n",
      "\u001b[32m2024-09-12 23:47:17\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Using 2 worker threads\n",
      "\u001b[32m2024-09-12 23:47:17\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Will stop early as soon as the QA threshold is met: 1250\n",
      "Generating:  53%|‚ñå| 666/1250 [1:25:47<1:12:37,  7.46s/qa, last tok/s=125, avg to\u001b[32m2024-09-13 01:13:26\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.982179 seconds\n",
      "Generating:  55%|‚ñå| 690/1250 [1:28:37<1:03:11,  6.77s/qa, last tok/s=203, avg to\u001b[32m2024-09-13 01:16:17\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.878425 seconds\n",
      "\u001b[32m2024-09-13 01:16:37\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.868982 seconds\n",
      "Generating:  56%|‚ñå| 694/1250 [1:29:48<1:33:35, 10.10s/qa, last tok/s=55.7, avg t\u001b[32m2024-09-13 01:17:41\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.829367 seconds\n",
      "\u001b[32m2024-09-13 01:17:49\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.814504 seconds\n",
      "Generating:  57%|‚ñå| 718/1250 [1:37:38<1:55:21, 13.01s/qa, last tok/s=137, avg to\u001b[32m2024-09-13 01:25:14\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.977118 seconds\n",
      "\u001b[32m2024-09-13 01:34:29\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.751392 seconds\n",
      "Generating:  59%|‚ñå| 742/1250 [1:50:00<2:03:04, 14.54s/qa, last tok/s=121, avg to\u001b[32m2024-09-13 01:37:49\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.792915 seconds\n",
      "\u001b[32m2024-09-13 01:53:54\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.838968 seconds\n",
      "Generating:  60%|‚ñå| 750/1250 [2:07:05<8:30:09, 61.22s/qa, last tok/s=132, avg to\u001b[32m2024-09-13 02:00:08\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.770171 seconds\n",
      "\u001b[32m2024-09-13 02:00:11\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.767828 seconds\n",
      "Generating:  62%|‚ñå| 774/1250 [2:15:06<2:09:51, 16.37s/qa, last tok/s=3.93e+4, av\u001b[32m2024-09-13 02:02:40\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.806417 seconds\n",
      "\u001b[32m2024-09-13 02:02:43\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.848643 seconds\n",
      "\u001b[32m2024-09-13 02:36:04\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.839746 seconds\n",
      "\u001b[32m2024-09-13 02:36:07\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.927263 seconds\n",
      "Generating:  63%|‚ñã| 786/1250 [2:49:49<10:47:42, 83.76s/qa, last tok/s=129, avg t\u001b[32m2024-09-13 02:37:25\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.752250 seconds\n",
      "\u001b[32m2024-09-13 02:37:28\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.878037 seconds\n",
      "Generating:  64%|‚ñã| 806/1250 [3:00:35<3:25:48, 27.81s/qa, last tok/s=604, avg to\u001b[32m2024-09-13 02:48:38\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.856927 seconds\n",
      "\u001b[32m2024-09-13 02:48:39\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.953919 seconds\n",
      "Generating:  66%|‚ñã| 826/1250 [3:10:55<2:12:17, 18.72s/qa, last tok/s=106, avg to\u001b[32m2024-09-13 02:58:42\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.898261 seconds\n",
      "\u001b[32m2024-09-13 02:58:47\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.862012 seconds\n",
      "Generating:  69%|‚ñã| 858/1250 [3:31:52<1:53:06, 17.31s/qa, last tok/s=104, avg to\u001b[32m2024-09-13 03:19:39\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.850202 seconds\n",
      "\u001b[32m2024-09-13 03:19:45\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.903047 seconds\n",
      "\u001b[32m2024-09-13 03:35:12\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 1.625042 seconds\n",
      "Generating:  78%|‚ñä| 970/1250 [4:01:42<29:43,  6.37s/qa, last tok/s=2.62e+4, avg \u001b[32m2024-09-13 03:49:16\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.903501 seconds\n",
      "\u001b[32m2024-09-13 03:49:18\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.800131 seconds\n",
      "\u001b[32m2024-09-13 03:51:42\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.882890 seconds\n",
      "\u001b[32m2024-09-13 04:07:43\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.961376 seconds\n",
      "Generating:  78%|‚ñä| 978/1250 [4:20:57<4:51:25, 64.28s/qa, last tok/s=144, avg to\u001b[32m2024-09-13 04:08:34\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.757965 seconds\n",
      "\u001b[32m2024-09-13 04:16:47\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.802413 seconds\n",
      "Generating:  79%|‚ñä| 982/1250 [4:30:05<6:24:37, 86.11s/qa, last tok/s=11.4, avg t\u001b[32m2024-09-13 04:17:35\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.830520 seconds\n",
      "\u001b[32m2024-09-13 04:17:41\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.817452 seconds\n",
      "Generating:  80%|‚ñä| 994/1250 [4:41:38<3:58:19, 55.86s/qa, last tok/s=6.46e+4, av\u001b[32m2024-09-13 04:29:50\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.894848 seconds\n",
      "\u001b[32m2024-09-13 04:29:54\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.832608 seconds\n",
      "Generating:  80%|‚ñä| 1003/1250 [4:46:23<2:42:46, 39.54s/qa, last tok/s=190, avg t\u001b[32m2024-09-13 04:34:00\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.984970 seconds\n",
      "\u001b[32m2024-09-13 04:34:02\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.813565 seconds\n",
      "Generating:  81%|‚ñä| 1015/1250 [5:02:59<3:20:39, 51.23s/qa, last tok/s=134, avg t\u001b[32m2024-09-13 05:05:41\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.778631 seconds\n",
      "\u001b[32m2024-09-13 05:05:48\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.963143 seconds\n",
      "Generating:  82%|‚ñä| 1019/1250 [5:18:38<6:45:31, 105.33s/qa, last tok/s=2.31, avg\u001b[32m2024-09-13 05:06:34\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.926199 seconds\n",
      "\u001b[32m2024-09-13 05:10:23\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.756817 seconds\n",
      "Generating:  82%|‚ñä| 1027/1250 [5:23:49<4:16:04, 68.90s/qa, last tok/s=146, avg t\u001b[32m2024-09-13 05:15:18\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.757173 seconds\n",
      "\u001b[32m2024-09-13 05:15:29\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.847671 seconds\n",
      "Generating:  83%|‚ñä| 1035/1250 [5:29:04<3:01:28, 50.64s/qa, last tok/s=216, avg t\u001b[32m2024-09-13 05:26:24\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.757629 seconds\n",
      "\u001b[32m2024-09-13 05:26:25\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.985702 seconds\n",
      "Generating:  85%|‚ñä| 1067/1250 [5:42:43<37:42, 12.36s/qa, last tok/s=1.89e+3, avg\u001b[32m2024-09-13 05:36:22\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.809436 seconds\n",
      "\u001b[32m2024-09-13 05:36:30\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.995975 seconds\n",
      "Generating:  87%|‚ñä| 1083/1250 [5:50:23<46:35, 16.74s/qa, last tok/s=159, avg tok\u001b[32m2024-09-13 05:38:41\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.750349 seconds\n",
      "\u001b[32m2024-09-13 05:45:00\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.865664 seconds\n",
      "Generating:  90%|‚ñâ| 1123/1250 [6:01:54<16:53,  7.98s/qa, last tok/s=2.84e+4, avg\u001b[32m2024-09-13 05:49:54\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.772773 seconds\n",
      "\u001b[32m2024-09-13 05:49:54\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.839508 seconds\n",
      "Generating:  91%|‚ñâ| 1139/1250 [6:12:15<37:02, 20.02s/qa, last tok/s=187, avg tok\u001b[32m2024-09-13 06:00:17\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.923359 seconds\n",
      "\u001b[32m2024-09-13 06:00:25\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.978873 seconds\n",
      "Generating:  93%|‚ñâ| 1159/1250 [6:31:23<1:09:42, 45.96s/qa, last tok/s=137, avg t\u001b[32m2024-09-13 06:18:51\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.899128 seconds\n",
      "\u001b[32m2024-09-13 06:18:58\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.896853 seconds\n",
      "Generating:  94%|‚ñâ| 1175/1250 [6:45:16<43:57, 35.17s/qa, last tok/s=115, avg tok\u001b[32m2024-09-13 06:32:46\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.828462 seconds\n",
      "\u001b[32m2024-09-13 06:32:52\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.920840 seconds\n",
      "Generating:  95%|‚ñâ| 1183/1250 [6:59:24<1:09:33, 62.29s/qa, last tok/s=128, avg t\u001b[32m2024-09-13 06:47:02\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.881848 seconds\n",
      "\u001b[32m2024-09-13 06:47:09\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.968068 seconds\n",
      "Generating:  98%|‚ñâ| 1223/1250 [7:19:20<04:36, 10.25s/qa, last tok/s=176, avg tok\u001b[32m2024-09-13 07:06:56\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.798307 seconds\n",
      "\u001b[32m2024-09-13 07:06:56\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.924557 seconds\n",
      "Generating:  99%|‚ñâ| 1239/1250 [7:34:28<05:05, 27.76s/qa, last tok/s=667, avg tok\u001b[32m2024-09-13 07:22:32\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.750293 seconds\n",
      "\u001b[32m2024-09-13 07:28:17\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.829952 seconds\n",
      "Generating: 100%|‚ñà| 1250/1250 [7:42:15<00:00, 30.88s/qa, last tok/s=97.9, avg to\u001b[32m2024-09-13 07:29:46\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Met threshold 1251 >= 1250 questions, stopping generation\n",
      "\u001b[32m2024-09-13 07:30:39\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mopenai._base_client\u001b[0m Retrying request to /chat/completions in 0.823142 seconds\n",
      "Generating: 100%|‚ñà| 1250/1250 [7:58:58<00:00, 22.99s/qa, last tok/s=97.9, avg to\n",
      "\u001b[32m2024-09-13 07:46:20\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Consumed 770377 prompt tokens, 259183 completion tokens, 1029560 total tokens\n",
      "Saving the dataset (1/1 shards): 100%|‚ñà| 1250/1250 [00:00<00:00, 7294.17 example\n",
      "Creating json from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 38.36ba/s]\n",
      "\u001b[32m2024-09-13 07:46:20\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Generated 1250 question/answer/CoT/documents samples\n",
      "\u001b[32m2024-09-13 07:46:20\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Dataset saved to /workspaces/llama-raft-recipe/dataset/surfing-1k\n",
      "\u001b[32m2024-09-13 07:46:20\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Done in 28751.76s\n"
     ]
    }
   ],
   "source": [
    "! [ ! -f $ds_output_file ] && env $(cat .env .env.state) python3 .gorilla/raft/raft.py \\\n",
    "    --datapath \"$doc_path\" \\\n",
    "    --output $ds_path \\\n",
    "    --distractors 3 \\\n",
    "    --doctype pdf \\\n",
    "    --chunk_size 512 \\\n",
    "    --questions 4 \\\n",
    "    --workers 2 \\\n",
    "    --system-prompt-key llama \\\n",
    "    --completion_model Meta-Llama-3-70B-Instruct \\\n",
    "    --embedding_model text-embedding-ada-002 \\\n",
    "    --qa-threshold $qa_threshold \\\n",
    "    || echo \"Dataset already generated, skipping generation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: The bit of shell logic wrapping the python script call allows to skip the generation if the dataset has already been generated so it is safe to run this notebook multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training, validation and evaluation splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define variables for the different files we will need throughout this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading arrow file dataset/surfing-1k/data-00000-of-00001.arrow\n"
     ]
    }
   ],
   "source": [
    "raft_arrow_file = f\"{ds_path}/data-00000-of-00001.arrow\"\n",
    "dataset_path = f\"{ds_path}-files/{ds_name}-full.jsonl\"\n",
    "dataset_path_hf = f\"{ds_path}-files/{ds_name}-hf.full.jsonl\"\n",
    "\n",
    "dataset_path_hf_train = f\"{ds_path}-files/{ds_name}-hf.train.jsonl\"\n",
    "dataset_path_hf_valid = f\"{ds_path}-files/{ds_name}-hf.valid.jsonl\"\n",
    "dataset_path_hf_eval = f\"{ds_path}-files/{ds_name}-hf.eval.jsonl\"\n",
    "\n",
    "dataset_path_ft_train = f\"{ds_path}-files/{ds_name}-ft.train.jsonl\"\n",
    "dataset_path_ft_valid = f\"{ds_path}-files/{ds_name}-ft.valid.jsonl\"\n",
    "\n",
    "print(f\"Reading arrow file {raft_arrow_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export dataset to JSONL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's export the Apache Arrow format file to JSONL, easier to manipulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train split: 1250 examples [00:00, 75399.15 examples/s]\n",
      "\u001b[32m2024-09-13 07:46:22\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mraft\u001b[0m Dataset has 1250 rows\n",
      "\u001b[32m2024-09-13 07:46:22\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mraft\u001b[0m Converting arrow file dataset/surfing-1k/data-00000-of-00001.arrow to jsonl hf file dataset/surfing-1k-files/surfing-1k-hf.full.jsonl\n",
      "Creating json from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 36.73ba/s]\n"
     ]
    }
   ],
   "source": [
    "! python .gorilla/raft/format.py \\\n",
    "    --input $raft_arrow_file \\\n",
    "    --output $dataset_path_hf \\\n",
    "    --output-format hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>oracle_context</th>\n",
       "      <th>cot_answer</th>\n",
       "      <th>instruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8d9f04a3-508d-4076-bdac-ea93e3b2ca76</td>\n",
       "      <td>general</td>\n",
       "      <td>What is windsurfing a combination of?</td>\n",
       "      <td>{'sentences': [['WindsurÔ¨Ång on Columbia River,...</td>\n",
       "      <td>WindsurÔ¨Ång on Columbia River,Oregon. Windsurfi...</td>\n",
       "      <td>To answer the question \"What is windsurfing a ...</td>\n",
       "      <td>&lt;DOCUMENT&gt;WindsurÔ¨Ång on Columbia River,Oregon....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f9536f4c-05db-4b36-b461-20b9885365ea</td>\n",
       "      <td>general</td>\n",
       "      <td>When did windsurfing become an Olympic sport?</td>\n",
       "      <td>{'sentences': [['Freestylesails are also flat ...</td>\n",
       "      <td>WindsurÔ¨Ång on Columbia River,Oregon. Windsurfi...</td>\n",
       "      <td>To answer the question, we need to identify th...</td>\n",
       "      <td>&lt;DOCUMENT&gt;Freestylesails are also flat when de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fd030ec0-2c7a-412f-8bc7-eff7fda045e7</td>\n",
       "      <td>general</td>\n",
       "      <td>Who invented and co-patented windsurfing in Ca...</td>\n",
       "      <td>{'sentences': [['Airton Cozzolino holds the re...</td>\n",
       "      <td>WindsurÔ¨Ång on Columbia River,Oregon. Windsurfi...</td>\n",
       "      <td>To answer the question, we need to identify th...</td>\n",
       "      <td>&lt;DOCUMENT&gt;Airton Cozzolino holds the record fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e9b232ad-25f1-4db8-9e1b-922e38a20d38</td>\n",
       "      <td>general</td>\n",
       "      <td>What are the major competitive disciplines in ...</td>\n",
       "      <td>{'sentences': [['Hamilton was eliminated inepi...</td>\n",
       "      <td>WindsurÔ¨Ång on Columbia River,Oregon. Windsurfi...</td>\n",
       "      <td>To answer the question, we need to identify th...</td>\n",
       "      <td>&lt;DOCUMENT&gt;Hamilton was eliminated inepisode 5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0bb5e901-32d5-45e7-a00d-d846fc0b7910</td>\n",
       "      <td>general</td>\n",
       "      <td>What are other names for the tube?</td>\n",
       "      <td>{'sentences': [['The equipment mustmatch the w...</td>\n",
       "      <td>Othernames for the tube include \"the barrel\", ...</td>\n",
       "      <td>To answer the question, we need to identify ot...</td>\n",
       "      <td>&lt;DOCUMENT&gt;The equipment mustmatch the wind con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id     type  \\\n",
       "0  8d9f04a3-508d-4076-bdac-ea93e3b2ca76  general   \n",
       "1  f9536f4c-05db-4b36-b461-20b9885365ea  general   \n",
       "2  fd030ec0-2c7a-412f-8bc7-eff7fda045e7  general   \n",
       "3  e9b232ad-25f1-4db8-9e1b-922e38a20d38  general   \n",
       "4  0bb5e901-32d5-45e7-a00d-d846fc0b7910  general   \n",
       "\n",
       "                                            question  \\\n",
       "0              What is windsurfing a combination of?   \n",
       "1      When did windsurfing become an Olympic sport?   \n",
       "2  Who invented and co-patented windsurfing in Ca...   \n",
       "3  What are the major competitive disciplines in ...   \n",
       "4                 What are other names for the tube?   \n",
       "\n",
       "                                             context  \\\n",
       "0  {'sentences': [['WindsurÔ¨Ång on Columbia River,...   \n",
       "1  {'sentences': [['Freestylesails are also flat ...   \n",
       "2  {'sentences': [['Airton Cozzolino holds the re...   \n",
       "3  {'sentences': [['Hamilton was eliminated inepi...   \n",
       "4  {'sentences': [['The equipment mustmatch the w...   \n",
       "\n",
       "                                      oracle_context  \\\n",
       "0  WindsurÔ¨Ång on Columbia River,Oregon. Windsurfi...   \n",
       "1  WindsurÔ¨Ång on Columbia River,Oregon. Windsurfi...   \n",
       "2  WindsurÔ¨Ång on Columbia River,Oregon. Windsurfi...   \n",
       "3  WindsurÔ¨Ång on Columbia River,Oregon. Windsurfi...   \n",
       "4  Othernames for the tube include \"the barrel\", ...   \n",
       "\n",
       "                                          cot_answer  \\\n",
       "0  To answer the question \"What is windsurfing a ...   \n",
       "1  To answer the question, we need to identify th...   \n",
       "2  To answer the question, we need to identify th...   \n",
       "3  To answer the question, we need to identify th...   \n",
       "4  To answer the question, we need to identify ot...   \n",
       "\n",
       "                                         instruction  \n",
       "0  <DOCUMENT>WindsurÔ¨Ång on Columbia River,Oregon....  \n",
       "1  <DOCUMENT>Freestylesails are also flat when de...  \n",
       "2  <DOCUMENT>Airton Cozzolino holds the record fo...  \n",
       "3  <DOCUMENT>Hamilton was eliminated inepisode 5....  \n",
       "4  <DOCUMENT>The equipment mustmatch the wind con...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_full_df = pd.read_json(dataset_path_hf, lines=True)\n",
    "hf_full_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hf_full_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, Markdown\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m randint\n\u001b[0;32m----> 4\u001b[0m sample_idx \u001b[38;5;241m=\u001b[39m randint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mhf_full_df\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m sample \u001b[38;5;241m=\u001b[39m hf_full_df\u001b[38;5;241m.\u001b[39miloc[sample_idx]\n\u001b[1;32m      6\u001b[0m instruction_md \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39minstruction\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<DOCUMENT>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`<DOCUMENT>`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</DOCUMENT>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`</DOCUMENT>`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hf_full_df' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from random import randint\n",
    "\n",
    "sample_idx = randint(0, len(hf_full_df) - 1)\n",
    "sample = hf_full_df.iloc[sample_idx]\n",
    "instruction_md = sample.instruction.replace(\"<DOCUMENT>\", \"`<DOCUMENT>`\").replace(\"</DOCUMENT>\", \"`</DOCUMENT>`\")\n",
    "oracle_context_md = sample.oracle_context.replace(\"<DOCUMENT>\", \"`<DOCUMENT>`\").replace(\"</DOCUMENT>\", \"`</DOCUMENT>`\")\n",
    "sample_answer_md = sample.cot_answer.replace(\"<ANSWER>\", \"`<ANSWER>`\").replace(\"##begin_quote##\", \"`##begin_quote##`\").replace(\"##end_quote##\", \"`##end_quote##`\")\n",
    "display(Markdown(f\"## Oracle Context\\n{oracle_context_md}\\n\\n## Question\\n{sample.question}\\n\\n## CoT Answer\\n{sample_answer_md}\\n\\n## Instruction\\n{instruction_md}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into train / validation / evaluation\n",
    "\n",
    "In machine learning, splitting a dataset into training, validation, and test sets is a fundamental step to ensure that your model is trained effectively, evaluated properly, and generalizes well to new data. Here‚Äôs a brief explanation of each split:\n",
    "\n",
    "- **Training Split**: The training set (80% of the data) is used to train the model. It helps the model learn patterns and relationships by adjusting its internal parameters based on input-output pairs.\n",
    "\n",
    "- **Validation Split**: The validation set (10%) is used during training to monitor the model‚Äôs performance and guide convergence. It helps fine-tune hyperparameters and ensures the model doesn‚Äôt overfit by providing feedback on unseen data during training.\n",
    "\n",
    "- **Evaluation Split (sometimes also called test split)**: The test set (10%) is used only after training is complete to evaluate the model‚Äôs final performance. It provides an unbiased measure of how well the model generalizes to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset at [1000, 1125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# split dataset into 80%/10%/10%\n",
    "import numpy as np\n",
    "\n",
    "samples_count = len(hf_full_df)\n",
    "splits = [int(finetuning_train_split * samples_count), int((finetuning_train_split + finetuning_valid_split) * samples_count)]\n",
    "print(f\"Splitting dataset at {splits}\")\n",
    "hf_train_df, hf_valid_df, hf_eval_df = np.split(hf_full_df, splits)\n",
    "hf_train_df.to_json(dataset_path_hf_train, orient=\"records\", lines=True)\n",
    "hf_valid_df.to_json(dataset_path_hf_valid, orient=\"records\", lines=True)\n",
    "hf_eval_df.to_json(dataset_path_hf_eval, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export training and validation splits into JSONL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train split: 1000 examples [00:00, 35757.98 examples/s]\n",
      "\u001b[32m2024-09-13 07:46:23\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mraft\u001b[0m Dataset has 1000 rows\n",
      "\u001b[32m2024-09-13 07:46:23\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mraft\u001b[0m Converting jsonl file dataset/surfing-1k-files/surfing-1k-hf.train.jsonl to jsonl chat file dataset/surfing-1k-files/surfing-1k-ft.train.jsonl\n",
      "Filter out empty examples: 100%|‚ñà‚ñà| 1000/1000 [00:00<00:00, 17219.62 examples/s]\n",
      "Rename fields and add  token: 100%|‚ñà| 1000/1000 [00:00<00:00, 25191.02 examples/\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 27966.50 examples/s]\n",
      "Creating json from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 39.27ba/s]\n"
     ]
    }
   ],
   "source": [
    "! python .gorilla/raft/format.py \\\n",
    "    --input $dataset_path_hf_train \\\n",
    "    --input-type jsonl \\\n",
    "    --output $dataset_path_ft_train \\\n",
    "    --output-format $format \\\n",
    "    --output-completion-prompt-column text\\\n",
    "    --output-completion-completion-column ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train split: 125 examples [00:00, 6334.81 examples/s]\n",
      "\u001b[32m2024-09-13 07:46:25\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mraft\u001b[0m Dataset has 125 rows\n",
      "\u001b[32m2024-09-13 07:46:25\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mraft\u001b[0m Converting jsonl file dataset/surfing-1k-files/surfing-1k-hf.valid.jsonl to jsonl chat file dataset/surfing-1k-files/surfing-1k-ft.valid.jsonl\n",
      "Filter out empty examples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:00<00:00, 7335.57 examples/s]\n",
      "Rename fields and add  token: 100%|‚ñà‚ñà| 125/125 [00:00<00:00, 4059.43 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:00<00:00, 5872.20 examples/s]\n",
      "Creating json from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 205.78ba/s]\n"
     ]
    }
   ],
   "source": [
    "! python .gorilla/raft/format.py \\\n",
    "    --input $dataset_path_hf_valid \\\n",
    "    --input-type jsonl \\\n",
    "    --output $dataset_path_ft_valid \\\n",
    "    --output-format $format \\\n",
    "    --output-completion-prompt-column text\\\n",
    "    --output-completion-completion-column ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'content': 'The following is a conversation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'content': 'The following is a conversation ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            messages\n",
       "0  [{'content': 'The following is a conversation ...\n",
       "1  [{'content': 'The following is a conversation ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path_ft_valid_df = pd.read_json(dataset_path_ft_valid, lines=True)\n",
    "dataset_path_ft_valid_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep the evaluation split aside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to format the evaluation dataset for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>oracle_context</th>\n",
       "      <th>cot_answer</th>\n",
       "      <th>instruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98c2b7cd-1026-4eff-9cf1-3afebb73e552</td>\n",
       "      <td>general</td>\n",
       "      <td>Is the emerging material lighter than traditio...</td>\n",
       "      <td>{'sentences': [['An emerging board material is...</td>\n",
       "      <td>An emerging board material is epoxy resinand E...</td>\n",
       "      <td>To answer the question, we need to determine i...</td>\n",
       "      <td>&lt;DOCUMENT&gt;An emerging board material is epoxy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>06d347e6-9663-4a8d-a5c2-148ddfce46e1</td>\n",
       "      <td>general</td>\n",
       "      <td>What type of surfing focuses on elegance and d...</td>\n",
       "      <td>{'sentences': [['Done for both exhibition and ...</td>\n",
       "      <td>Done for both exhibition and competitionsthe g...</td>\n",
       "      <td>To answer the question, we need to identify th...</td>\n",
       "      <td>&lt;DOCUMENT&gt;Done for both exhibition and competi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id     type  \\\n",
       "0  98c2b7cd-1026-4eff-9cf1-3afebb73e552  general   \n",
       "1  06d347e6-9663-4a8d-a5c2-148ddfce46e1  general   \n",
       "\n",
       "                                            question  \\\n",
       "0  Is the emerging material lighter than traditio...   \n",
       "1  What type of surfing focuses on elegance and d...   \n",
       "\n",
       "                                             context  \\\n",
       "0  {'sentences': [['An emerging board material is...   \n",
       "1  {'sentences': [['Done for both exhibition and ...   \n",
       "\n",
       "                                      oracle_context  \\\n",
       "0  An emerging board material is epoxy resinand E...   \n",
       "1  Done for both exhibition and competitionsthe g...   \n",
       "\n",
       "                                          cot_answer  \\\n",
       "0  To answer the question, we need to determine i...   \n",
       "1  To answer the question, we need to identify th...   \n",
       "\n",
       "                                         instruction  \n",
       "0  <DOCUMENT>An emerging board material is epoxy ...  \n",
       "1  <DOCUMENT>Done for both exhibition and competi...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json(dataset_path_hf_eval, lines=True).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
