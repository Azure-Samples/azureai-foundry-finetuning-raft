{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1714204",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "### Define variables we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31004ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# User provided values\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# Variables passed by previous notebooks\n",
    "load_dotenv(\".env.state\")\n",
    "\n",
    "# Let's capture the initial working directory because the evaluate function will change it\n",
    "dir = os.getcwd()\n",
    "\n",
    "experiment_name = os.getenv(\"DATASET_NAME\")\n",
    "experiment_dir = f\"{dir}/dataset/{experiment_name}-files\"\n",
    "\n",
    "# Dataset generated by the gen notebook that we will evaluate the baseline and finetuned models on\n",
    "dataset_path_hf_eval = f\"{experiment_dir}/{experiment_name}-hf.eval.jsonl\"\n",
    "\n",
    "# Evaluated answer files\n",
    "dataset_path_hf_eval_answer = f\"{experiment_dir}/{experiment_name}-hf.eval.answer.jsonl\"\n",
    "dataset_path_hf_eval_answer_baseline = f\"{experiment_dir}/{experiment_name}-hf.eval.answer.baseline.jsonl\"\n",
    "\n",
    "# Formatted answer evaluation files\n",
    "dataset_path_eval_answer_finetuned = f\"{experiment_dir}/{experiment_name}-eval.answer.finetuned.jsonl\"\n",
    "dataset_path_eval_answer_baseline = f\"{experiment_dir}/{experiment_name}-eval.answer.baseline.jsonl\"\n",
    "\n",
    "# Scored answer files\n",
    "dataset_path_eval_answer_score_finetuned = f\"{experiment_dir}/{experiment_name}-eval.answer.score.finetuned.jsonl\"\n",
    "dataset_path_eval_answer_score_baseline = f\"{experiment_dir}/{experiment_name}-eval.answer.score.baseline.jsonl\"\n",
    "\n",
    "BASELINE_OPENAI_DEPLOYMENT = os.getenv(\"BASELINE_OPENAI_DEPLOYMENT\")\n",
    "FINETUNED_OPENAI_DEPLOYMENT = os.getenv(\"FINETUNED_OPENAI_DEPLOYMENT\")\n",
    "FINETUNED_MODEL_FORMAT = os.getenv(\"FINETUNED_MODEL_FORMAT\")\n",
    "\n",
    "print(f\"Evaluating the finetuned {FINETUNED_MODEL_FORMAT} model {FINETUNED_OPENAI_DEPLOYMENT} against the baseline model {BASELINE_OPENAI_DEPLOYMENT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419a71e0",
   "metadata": {},
   "source": [
    "## Let's review the formatted files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c556e",
   "metadata": {},
   "source": [
    "### Finetuned model answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c0c88ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff092e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finetuned = pd.read_json(dataset_path_eval_answer_finetuned, lines=True)\n",
    "df_finetuned.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b966d32",
   "metadata": {},
   "source": [
    "### Baseline model answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b4121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline = pd.read_json(dataset_path_eval_answer_baseline, lines=True)\n",
    "df_baseline.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84630abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged=pd.merge(df_baseline, df_finetuned, on=\"question\", suffixes=('_baseline', '_finetuned'))\n",
    "df_merged.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c28353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "sample_idx = randint(0, len(df_merged) - 1)\n",
    "sample = df_merged.iloc[sample_idx]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c3f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def format_tags_md(text):\n",
    "    md = text.replace(\"<ANSWER>\", \"`<ANSWER>`\").replace(\"<DOCUMENT>\", \"`<DOCUMENT>`\").replace(\"</DOCUMENT>\", \"`</DOCUMENT>`\").replace(\"##begin_quote##\", \"`##begin_quote##`\").replace(\"##end_quote##\", \"`##end_quote##`\")\n",
    "    return md\n",
    "\n",
    "answer_baseline_md = format_tags_md(sample.answer_baseline)\n",
    "answer_finetuned_md = format_tags_md(sample.answer_finetuned)\n",
    "context_md = format_tags_md(sample.context_finetuned)\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "## Context\n",
    "{context_md}\n",
    "\n",
    "## Question\n",
    "{sample.question}\n",
    "\n",
    "## Baseline Answer\n",
    "{answer_baseline_md}\n",
    "\n",
    "## Finetuned CoT Answer\n",
    "{answer_finetuned_md}\n",
    "\n",
    "## Finetuned Answer\n",
    "{sample.final_answer_finetuned}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f72d3d",
   "metadata": {},
   "source": [
    "#### Quality Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a219acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from promptflow.core import AzureOpenAIModelConfiguration\n",
    "\n",
    "azure_endpoint = os.environ.get(\"JUDGE_AZURE_OPENAI_ENDPOINT\")\n",
    "azure_deployment = os.environ.get(\"JUDGE_AZURE_OPENAI_DEPLOYMENT\")\n",
    "api_key = os.environ.get(\"JUDGE_AZURE_OPENAI_API_KEY\")\n",
    "api_version = os.environ.get(\"JUDGE_OPENAI_API_VERSION\")\n",
    "\n",
    "print(f\"azure_endpoint={azure_endpoint}\")\n",
    "print(f\"azure_deployment={azure_deployment}\")\n",
    "print(f\"api_version={api_version}\")\n",
    "\n",
    "# Initialize Azure OpenAI Connection\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    azure_deployment=azure_deployment,\n",
    "    api_version=api_version,\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8965ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluators import RelevanceEvaluator, SimilarityEvaluator, GroundednessEvaluator\n",
    "\n",
    "# Initializing evaluators\n",
    "similarity = SimilarityEvaluator(model_config)\n",
    "groundedness = GroundednessEvaluator(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6247d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(dataset_path_eval_answer_finetuned, lines=True)\n",
    "sample = df.iloc[1]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad14f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sample_metrics(sample):\n",
    "    similarity_score = similarity(\n",
    "        question=sample[\"question\"],\n",
    "        answer=sample[\"final_answer\"],\n",
    "        context=sample[\"context\"],\n",
    "        ground_truth=sample[\"gold_final_answer\"],\n",
    "    )\n",
    "    groundedness_score = groundedness(\n",
    "        answer=sample[\"final_answer\"],\n",
    "        context=sample[\"context\"],\n",
    "    )\n",
    "    return similarity_score | groundedness_score\n",
    "\n",
    "compute_sample_metrics(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846babb1-59b5-4d38-bb3a-d6eebd39ebee",
   "metadata": {},
   "source": [
    "### Using the Evaluate API to calculate the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe3fa2",
   "metadata": {},
   "source": [
    "In previous sections, we walked you through how to use built-in evaluators to evaluate a single row and how to define your own custom evaluators. Now, we will show you how to use these evaluators with the powerful `evaluate` API to assess an entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049643c",
   "metadata": {},
   "source": [
    "### Running the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fcfc57",
   "metadata": {},
   "source": [
    "Now, we will invoke the `evaluate` API using a few evaluators that we already initialized\n",
    "\n",
    "Additionally, we have a column mapping to map the `truth` column from the dataset to `ground_truth`, which is accepted by the evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b03e557-19bd-4a2a-ae3f-bbaf6846fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluate import evaluate\n",
    "\n",
    "def score_dataset(dataset, output_path=None):\n",
    "    result = evaluate(\n",
    "        data=dataset,\n",
    "        evaluators={\"similarity\": similarity, \"groundedness\": groundedness},\n",
    "        # column mapping\n",
    "        evaluator_config={\n",
    "            \"similarity\": {\n",
    "                \"question\": \"${data.question}\",\n",
    "                \"answer\": \"${data.final_answer}\",\n",
    "                \"ground_truth\": \"${data.gold_final_answer}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "            },\n",
    "            \"groundedness\": {\n",
    "                \"answer\": \"${data.final_answer}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if output_path:\n",
    "        pd.DataFrame.from_dict(result[\"rows\"]).to_json(output_path, orient=\"records\", lines=True)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d51dd4c",
   "metadata": {},
   "source": [
    "#### Baseline model evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4059934",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json(dataset_path_eval_answer_baseline, lines=True).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c2c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_result = score_dataset(dataset_path_eval_answer_baseline, dataset_path_eval_answer_score_baseline)\n",
    "from IPython.display import display, JSON\n",
    "\n",
    "display(JSON(baseline_result[\"metrics\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd3c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results using Azure AI Studio UI\n",
    "if baseline_result[\"studio_url\"]:\n",
    "    print(f\"Results uploaded to AI Studio {baseline_result['studio_url']}\")\n",
    "else:\n",
    "    print(\"Results available at http://127.0.0.1:23333\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404ec34",
   "metadata": {},
   "source": [
    "#### Finetuned model evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6185474",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json(dataset_path_eval_answer_finetuned, lines=True).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3367eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_result = score_dataset(dataset_path_eval_answer_finetuned, dataset_path_eval_answer_score_finetuned)\n",
    "from IPython.display import display, JSON\n",
    "\n",
    "display(JSON(finetune_result[\"metrics\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0fb00",
   "metadata": {},
   "source": [
    "\n",
    "Finally, let's check the results produced by the evaluate API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb28f9a-1f9c-4c2e-8b32-e7da51585f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results using Azure AI Studio UI\n",
    "if finetune_result[\"studio_url\"]:\n",
    "    print(f\"Results uploaded to AI Studio {finetune_result['studio_url']}\")\n",
    "else:\n",
    "    print(\"Results available at http://127.0.0.1:23333\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb78572d",
   "metadata": {},
   "source": [
    "## Let's look at examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline=pd.read_json(dataset_path_eval_answer_baseline, lines=True)\n",
    "df_finetuned=pd.read_json(dataset_path_eval_answer_finetuned, lines=True)\n",
    "df_merged=pd.merge(df_baseline, df_finetuned, on=\"question\", suffixes=('_baseline', '_finetuned'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79181e85",
   "metadata": {},
   "source": [
    "## Compare the metrics of the fine-tuned model against the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325762f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame.from_dict({\"baseline\": baseline_result[\"metrics\"], \"finetuned\": finetune_result[\"metrics\"]})\n",
    "metrics[\"improvement\"] = metrics[\"finetuned\"] / metrics[\"baseline\"]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0669cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.drop(\"improvement\", axis=1).plot.bar(rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e1d349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
